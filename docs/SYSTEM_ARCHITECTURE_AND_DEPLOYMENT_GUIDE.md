# ğŸ“ RAG3 EÄŸitim Sistemi - KapsamlÄ± Sistem Mimarisi ve Deployment Rehberi

## ğŸ“‹ Ä°Ã§indekiler

1. [Sistem Genel BakÄ±ÅŸ](#sistem-genel-bakÄ±ÅŸ)
2. [Mimari YapÄ±](#mimari-yapÄ±)
3. [Teknoloji Stack](#teknoloji-stack)
4. [Servis DetaylarÄ±](#servis-detaylarÄ±)
5. [Docker Deployment](#docker-deployment)
6. [Kurulum Rehberi](#kurulum-rehberi)
7. [API DokÃ¼mantasyonu](#api-dokÃ¼mantasyonu)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Sistem Genel BakÄ±ÅŸ

**RAG3 EÄŸitim Sistemi**, Ã¶ÄŸretmenlerin ders materyallerini yÃ¼kleyip Ã¶ÄŸrencilerinin bu materyaller Ã¼zerinde akÄ±llÄ± sorular sormasÄ±nÄ± saÄŸlayan bir **Retrieval Augmented Generation (RAG)** tabanlÄ± eÄŸitim platformudur.

### ğŸŒŸ Ana Ã–zellikler

- ğŸ“š **Ders Oturumu YÃ¶netimi**: SÄ±nÄ±f bazlÄ± materyal organizasyonu
- ğŸ“„ **Ã‡oklu Format DesteÄŸi**: PDF, DOCX, PPTX, XLSX, MD dosyalarÄ±
- ğŸ¤– **AI Destekli Soru-Cevap**: GPT tabanlÄ± akÄ±llÄ± asistan
- ğŸ” **Kaynak GÃ¶sterimi**: CevaplarÄ±n hangi belgelerden geldiÄŸini gÃ¶sterir
- ğŸ¨ **Modern UI/UX**: EÄŸitim odaklÄ±, kullanÄ±cÄ± dostu arayÃ¼z
- ğŸ³ **Docker Deployment**: Kolay kurulum ve yÃ¶netim

---

## ğŸ—ï¸ Mimari YapÄ±

### ğŸ“Š Sistem Mimarisi DiyagramÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   API Gateway   â”‚    â”‚  Microservices  â”‚
â”‚   (Next.js)     â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   Ecosystem     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Teacher Panel â”‚    â”‚ â€¢ Route Mgmt    â”‚    â”‚ â€¢ Doc Processor â”‚
â”‚ â€¢ Student Chat  â”‚    â”‚ â€¢ Session Mgmt  â”‚    â”‚ â€¢ Model Inferenceâ”‚
â”‚ â€¢ File Upload   â”‚    â”‚ â€¢ Auth Layer    â”‚    â”‚ â€¢ DocStrange    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Data Layer    â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ ChromaDB      â”‚
                    â”‚ â€¢ SQLite        â”‚
                    â”‚ â€¢ File Storage  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Veri AkÄ±ÅŸÄ±

1. **ğŸ“¤ Dosya YÃ¼kleme**: Ã–ÄŸretmen â†’ Frontend â†’ API Gateway â†’ DocStrange/DocProcessor
2. **ğŸ”„ Ä°ÅŸleme**: DocProcessor â†’ Chunking â†’ Embedding â†’ ChromaDB
3. **â“ Soru Sorma**: Ã–ÄŸrenci â†’ Frontend â†’ API Gateway â†’ Model Inference
4. **ğŸ” RAG Ä°ÅŸlemi**: Model Inference â†’ ChromaDB â†’ LLM â†’ Cevap
5. **ğŸ“‹ Kaynak GÃ¶sterimi**: Cevap + Kaynak Belgeler â†’ Frontend

---

## ğŸ’» Teknoloji Stack

### ğŸ¨ Frontend
- **Framework**: Next.js 14 (React 18)
- **Styling**: Tailwind CSS
- **Language**: TypeScript
- **Build**: Standalone mode for Docker

### ğŸš€ Backend
- **API Gateway**: FastAPI (Python 3.11)
- **Session Management**: SQLite + Professional Session Manager
- **Authentication**: JWT-based (basit implementasyon)

### ğŸ¤– AI & ML
- **LLM Provider**: Groq API (Llama 3.1, Mixtral, Gemma)
- **Local LLM**: Ollama (opsiyonel)
- **Embeddings**: mxbai-embed-large (Ollama)
- **Vector Database**: ChromaDB 1.3.0

### ğŸ“„ Document Processing
- **PDF Processing**: DocStrange API
- **Office Docs**: python-docx, python-pptx, openpyxl
- **Chunking**: Semantic chunking (1500 chars, 150 overlap)

### ğŸ³ Infrastructure
- **Containerization**: Docker + Docker Compose
- **Orchestration**: Docker Compose
- **Storage**: Named volumes
- **Networking**: Bridge network

---

## ğŸ”§ Servis DetaylarÄ±

### 1. ğŸŒ Frontend Service (`rag3-frontend`)
```yaml
Port: 3000
Technology: Next.js 14
Environment:
  - NEXT_PUBLIC_API_URL=http://host.docker.internal:8000
  - NODE_ENV=development
```

**Ã–zellikler**:
- ğŸ“ EÄŸitim odaklÄ± modern UI
- ğŸ“± Responsive tasarÄ±m
- ğŸ”„ Real-time chat interface
- ğŸ“Š Dashboard ve analytics
- ğŸ“ Dosya yÃ¼kleme arayÃ¼zÃ¼

### 2. ğŸšª API Gateway (`api-gateway`)
```yaml
Port: 8000 (internal: 8080)
Technology: FastAPI
Database: SQLite (sessions)
```

**Endpoints**:
- `POST /sessions` - Ders oturumu oluÅŸtur
- `GET /sessions` - OturumlarÄ± listele
- `POST /documents/convert` - Belge dÃ¶nÃ¼ÅŸtÃ¼rme
- `POST /documents/upload-markdown` - MD dosyasÄ± yÃ¼kle
- `POST /documents/process-and-store` - RAG iÅŸleme
- `POST /rag/query` - Soru-cevap

### 3. ğŸ“„ Document Processing Service (`document-processing-service`)
```yaml
Port: 8003 (internal: 8080)
Technology: FastAPI + Python
Dependencies: ChromaDB, Model Inference
```

**Ä°ÅŸlevler**:
- ğŸ“ Metin Ã§Ä±karma ve temizleme
- âœ‚ï¸ Semantic chunking
- ğŸ”¢ Embedding oluÅŸturma
- ğŸ’¾ ChromaDB'ye kaydetme

### 4. ğŸ¤– Model Inference Service (`model-inference-service`)
```yaml
Port: 8002
Technology: FastAPI + Ollama
Models: llama3:8b, nomic-embed-text
```

**Ã–zellikler**:
- ğŸŒ Groq API entegrasyonu
- ğŸ  Local Ollama desteÄŸi
- ğŸ“Š Embedding generation
- ğŸ”„ Model switching

### 5. ğŸ“‹ DocStrange Service (`docstrange-service`)
```yaml
Port: 8005 (internal: 80)
Technology: FastAPI
API: DocStrange API
```

**Desteklenen Formatlar**:
- ğŸ“„ PDF â†’ Markdown
- ğŸ“ DOCX â†’ Markdown
- ğŸ“Š PPTX â†’ Markdown
- ğŸ“ˆ XLSX â†’ Markdown

### 6. ğŸ—ƒï¸ ChromaDB Service (`chromadb-service`)
```yaml
Port: 8004 (internal: 8000)
Technology: ChromaDB 1.3.0
Storage: Persistent volume
```

**YapÄ±landÄ±rma**:
- ğŸ’¾ Persistent storage: `/data`
- ğŸ”’ Authentication: Disabled
- ğŸ“Š Telemetry: Disabled
- ğŸ”„ Reset capability: Enabled

---

## ğŸ³ Docker Deployment

### ğŸ“ Dosya YapÄ±sÄ±
```
rag3_for_colab/
â”œâ”€â”€ docker-compose.yml          # Ana orchestration
â”œâ”€â”€ Dockerfile.gateway.local    # API Gateway
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ Dockerfile.frontend     # Frontend
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ docstrange_service/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ document_processing_service/
â”‚   â”‚   â””â”€â”€ Dockerfile.local
â”‚   â””â”€â”€ model_inference_service/
â”‚       â””â”€â”€ Dockerfile.local
â””â”€â”€ src/                        # API Gateway source
```

### ğŸš€ Deployment KomutlarÄ±

#### 1. ğŸ—ï¸ Ä°lk Kurulum
```bash
# Repository'yi klonla
git clone <repository-url>
cd rag3_for_colab

# Environment dosyalarÄ±nÄ± kontrol et
# docker-compose.yml iÃ§indeki API key'leri gÃ¼ncelle

# TÃ¼m servisleri build et
docker-compose build

# Servisleri baÅŸlat
docker-compose up -d
```

#### 2. ğŸ”„ GÃ¼ncelleme
```bash
# Servisleri durdur
docker-compose down

# Yeni kod deÄŸiÅŸikliklerini Ã§ek
git pull

# DeÄŸiÅŸen servisleri rebuild et
docker-compose build

# Servisleri yeniden baÅŸlat
docker-compose up -d
```

#### 3. ğŸ“Š Monitoring
```bash
# Servis durumlarÄ±nÄ± kontrol et
docker-compose ps

# LoglarÄ± gÃ¶rÃ¼ntÃ¼le
docker-compose logs -f [service-name]

# Kaynak kullanÄ±mÄ±nÄ± kontrol et
docker stats
```

### ğŸ’¾ Volume YÃ¶netimi

```yaml
volumes:
  ollama_cache:      # Ollama model cache
  chroma_data:       # ChromaDB persistent data
  session_data:      # Session database
  markdown_data:     # Uploaded markdown files
```

**Backup Komutu**:
```bash
# Volume backup
docker run --rm -v rag3_for_colab_chroma_data:/data -v $(pwd):/backup alpine tar czf /backup/chroma_backup.tar.gz -C /data .
```

---

## ğŸ› ï¸ Kurulum Rehberi

### ğŸ“‹ Gereksinimler

- ğŸ³ **Docker**: 20.10+
- ğŸ™ **Docker Compose**: 2.0+
- ğŸ’¾ **RAM**: Minimum 8GB (Ã–nerilen 16GB)
- ğŸ’¿ **Disk**: Minimum 20GB boÅŸ alan
- ğŸŒ **Network**: Ä°nternet baÄŸlantÄ±sÄ± (model indirme iÃ§in)

### ğŸ”§ AdÄ±m AdÄ±m Kurulum

#### 1. ğŸ“¥ Repository HazÄ±rlama
```bash
git clone <repository-url>
cd rag3_for_colab
```

#### 2. ğŸ”‘ API Key YapÄ±landÄ±rmasÄ±
`docker-compose.yml` dosyasÄ±nda:
```yaml
environment:
  - GROQ_API_KEY=your_groq_api_key_here
  - DOCSTRANGE_API_KEY=your_docstrange_key_here
```

#### 3. ğŸ—ï¸ Build ve Deploy
```bash
# TÃ¼m servisleri build et
docker-compose build --no-cache

# Servisleri baÅŸlat
docker-compose up -d

# Durumu kontrol et
docker-compose ps
```

#### 4. âœ… DoÄŸrulama
- ğŸŒ Frontend: http://localhost:3000
- ğŸšª API Gateway: http://localhost:8000/health
- ğŸ—ƒï¸ ChromaDB: http://localhost:8004/api/v1/heartbeat

### ğŸ”§ GeliÅŸtirme OrtamÄ±

#### Hot Reload iÃ§in:
```bash
# Frontend development
cd frontend
npm run dev

# Backend development (API Gateway)
cd src
uvicorn api.main:app --reload --host 0.0.0.0 --port 8080
```

---

## ğŸ“š API DokÃ¼mantasyonu

### ğŸ” Authentication
Basit token-based authentication:
```javascript
localStorage.setItem("isAuthenticated", "true");
```

### ğŸ“‹ Session Management

#### Ders Oturumu OluÅŸturma
```http
POST /sessions
Content-Type: application/json

{
  "name": "Biyoloji 9. SÄ±nÄ±f",
  "description": "HÃ¼cre bÃ¶lÃ¼nmesi konusu",
  "category": "biology",
  "created_by": "teacher_id",
  "grade_level": "9",
  "subject_area": "Biyoloji"
}
```

#### OturumlarÄ± Listeleme
```http
GET /sessions?limit=10&category=biology
```

### ğŸ“„ Document Processing

#### Belge DÃ¶nÃ¼ÅŸtÃ¼rme
```http
POST /documents/convert
Content-Type: multipart/form-data

file: [PDF/DOCX/PPTX/XLSX file]
```

#### Markdown YÃ¼kleme
```http
POST /documents/upload-markdown
Content-Type: multipart/form-data

file: [.md file]
```

#### RAG Ä°ÅŸleme
```http
POST /documents/process-and-store
Content-Type: multipart/form-data

session_id: "session-uuid"
markdown_files: ["file1.md", "file2.md"]
chunk_strategy: "semantic"
chunk_size: 1500
chunk_overlap: 150
embedding_model: "mxbai-embed-large"
```

### ğŸ¤– RAG Query

#### Soru Sorma
```http
POST /rag/query
Content-Type: application/json

{
  "session_id": "session-uuid",
  "query": "HÃ¼cre bÃ¶lÃ¼nmesi nasÄ±l gerÃ§ekleÅŸir?",
  "top_k": 5,
  "use_rerank": true,
  "min_score": 0.1,
  "max_context_chars": 8000,
  "model": "llama-3.1-8b-instant"
}
```

**Response**:
```json
{
  "answer": "HÃ¼cre bÃ¶lÃ¼nmesi...",
  "sources": [
    {
      "content": "Ä°lgili metin parÃ§asÄ±...",
      "metadata": {
        "source_file": "biyoloji_ders_notu.md",
        "chunk_id": "chunk_123"
      },
      "score": 0.85
    }
  ]
}
```

---

## ğŸ”§ Troubleshooting

### ğŸš¨ YaygÄ±n Sorunlar

#### 1. ğŸ³ Docker Build HatalarÄ±
```bash
# Cache temizleme
docker system prune -a

# Specific service rebuild
docker-compose build --no-cache [service-name]
```

#### 2. ğŸ”Œ Port Ã‡akÄ±ÅŸmasÄ±
```bash
# Port kullanÄ±mÄ±nÄ± kontrol et
netstat -tulpn | grep :3000

# Alternatif port kullan
docker-compose up -d --scale frontend=0
docker run -p 3001:3000 rag3_for_colab-frontend
```

#### 3. ğŸ¤– Ollama BaÄŸlantÄ± Sorunu
```bash
# Container iÃ§inde Ollama durumunu kontrol et
docker exec model-inference-service curl http://127.0.0.1:11434/api/tags

# Model indirme
docker exec model-inference-service ollama pull llama3:8b
```

#### 4. ğŸ’¾ ChromaDB Veri KaybÄ±
```bash
# Volume durumunu kontrol et
docker volume ls | grep chroma

# Backup'tan geri yÃ¼kleme
docker run --rm -v rag3_for_colab_chroma_data:/data -v $(pwd):/backup alpine tar xzf /backup/chroma_backup.tar.gz -C /data
```

#### 5. ğŸŒ Frontend Build HatalarÄ±
```bash
# Node modules temizleme
docker-compose exec frontend rm -rf node_modules .next
docker-compose build frontend --no-cache
```

### ğŸ“Š Performance Tuning

#### Memory Optimization
```yaml
# docker-compose.yml
services:
  model-inference-service:
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
```

#### ChromaDB Optimization
```yaml
chromadb-service:
  environment:
    - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
    - CHROMA_SERVER_HTTP_PORT=8000
    - ANONYMIZED_TELEMETRY=false
```

---

## ğŸ“ˆ Monitoring ve Logs

### ğŸ“Š Health Checks
```bash
# API Gateway
curl http://localhost:8000/health

# ChromaDB
curl http://localhost:8004/api/v1/heartbeat

# Model Inference
curl http://localhost:8002/models/available
```

### ğŸ“ Log Monitoring
```bash
# TÃ¼m servislerin loglarÄ±
docker-compose logs -f

# Specific service logs
docker-compose logs -f api-gateway
docker-compose logs -f frontend
docker-compose logs -f model-inference-service
```

### ğŸ” Debug Mode
```yaml
# docker-compose.yml debug configuration
environment:
  - DEBUG=true
  - LOG_LEVEL=DEBUG
```

---

## ğŸš€ Production Deployment

### ğŸ”’ Security Checklist
- [ ] API key'leri environment variables'a taÅŸÄ±
- [ ] HTTPS sertifikasÄ± ekle
- [ ] CORS ayarlarÄ±nÄ± gÃ¼ncelle
- [ ] Rate limiting ekle
- [ ] Authentication gÃ¼Ã§lendir

### ğŸ“Š Scaling Considerations
- ğŸ”„ Load balancer ekle
- ğŸ“ˆ Horizontal scaling iÃ§in Kubernetes
- ğŸ’¾ External database (PostgreSQL)
- ğŸ—ƒï¸ Distributed ChromaDB setup

---

## ğŸ“ Destek ve KatkÄ±

### ğŸ› Bug Reporting
Issues aÃ§arken ÅŸunlarÄ± ekleyin:
- ğŸ³ Docker version
- ğŸ’» OS bilgisi
- ğŸ“ Error logs
- ğŸ”„ Reproduction steps

### ğŸ¤ Contribution Guidelines
1. Fork repository
2. Feature branch oluÅŸtur
3. Changes commit et
4. Pull request aÃ§
5. Review sÃ¼recini bekle

---

## ğŸ“„ Lisans

Bu proje **MIT License** altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

---

## ğŸ‘¨â€ğŸ’» GeliÅŸtirici

**Engin DALGA** - Burdur Mehmet Akif Ersoy Ãœniversitesi YÃ¼ksek Lisans Ã–devi

ğŸ“§ Contact: [email]
ğŸ”— LinkedIn: [profile]
ğŸ™ GitHub: [profile]

---

*Son gÃ¼ncelleme: KasÄ±m 2024*
