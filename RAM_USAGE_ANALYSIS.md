# ğŸ’¾ RAM KullanÄ±m Analizi - 8GB Sunucu

## ğŸ“Š GerÃ§ek RAM KullanÄ±m Tahmini

### Docker Resource Limits (Maksimum)
Limit'ler **maksimum** deÄŸerlerdir. GerÃ§ek kullanÄ±m genellikle %50-70 arasÄ±dÄ±r.

| Servis | Limit | GerÃ§ek KullanÄ±m (Tahmini) |
|--------|-------|---------------------------|
| API Gateway | 2GB | ~800MB - 1.2GB |
| APRAG Service | 2GB | ~600MB - 1GB |
| Auth Service | 1GB | ~200MB - 400MB |
| Document Processing | 3GB | ~800MB - 1.5GB |
| Model Inference | 2GB | ~300MB - 600MB |
| Ollama | 8GB | ~2GB - 4GB (model yÃ¼klÃ¼yse) |
| ChromaDB | 2GB | ~500MB - 1GB |
| Reranker Service | 2GB | ~200MB - 400MB |
| Frontend | 1GB | ~200MB - 400MB |
| **TOPLAM LIMIT** | **23GB** | **~5.5GB - 10GB** |

### âš ï¸ Ã–NEMLÄ°: Ollama RAM KullanÄ±mÄ±

Ollama'nÄ±n RAM kullanÄ±mÄ± **yÃ¼klenen modele gÃ¶re deÄŸiÅŸir**:
- **KÃ¼Ã§Ã¼k model** (7B): ~2-3GB
- **Orta model** (13B): ~4-6GB
- **BÃ¼yÃ¼k model** (70B): ~8GB+

### ğŸ¯ 8GB Sunucu Ä°Ã§in Ã–neriler

#### Senaryo 1: Ollama KullanÄ±lmÄ±yorsa (Cloud LLM)
- **Toplam RAM**: ~5-7GB âœ… **8GB YETERLÄ°**
- **Buffer**: ~1-3GB

#### Senaryo 2: Ollama KÃ¼Ã§Ã¼k Model (7B)
- **Toplam RAM**: ~7-10GB âš ï¸ **8GB SINIRDA**
- **Buffer**: ~0-1GB (riskli)

#### Senaryo 3: Ollama Orta/BÃ¼yÃ¼k Model
- **Toplam RAM**: ~10-15GB âŒ **8GB YETERSÄ°Z**

## ğŸ”§ Optimizasyon Ã–nerileri

### 1. Resource Limits'i DÃ¼ÅŸÃ¼r (8GB iÃ§in)
```yaml
# docker-compose.prod.yml iÃ§inde:

api-gateway:
  deploy:
    resources:
      limits:
        memory: 1.5G  # 2GB'dan dÃ¼ÅŸÃ¼r
      reservations:
        memory: 400M

aprag-service:
  deploy:
    resources:
      limits:
        memory: 1.5G  # 2GB'dan dÃ¼ÅŸÃ¼r
      reservations:
        memory: 400M

document-processing-service:
  deploy:
    resources:
      limits:
        memory: 2G  # 3GB'dan dÃ¼ÅŸÃ¼r
      reservations:
        memory: 800M

ollama-service:
  deploy:
    resources:
      limits:
        memory: 4G  # 8GB'dan dÃ¼ÅŸÃ¼r (kÃ¼Ã§Ã¼k model iÃ§in)
      reservations:
        memory: 1G
```

### 2. Ollama KullanmÄ±yorsanÄ±z
EÄŸer sadece cloud LLM (Groq, Alibaba, DeepSeek) kullanÄ±yorsanÄ±z:
- Ollama container'Ä±nÄ± **kapatÄ±n**
- RAM kullanÄ±mÄ±: ~5-6GB âœ…

### 3. Worker SayÄ±larÄ±nÄ± Optimize Et
```yaml
# Daha az worker = daha az RAM

api-gateway:
  command: python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 3  # 5'ten 3'e

aprag-service:
  command: python -m uvicorn main:app --host 0.0.0.0 --port 8007 --workers 2  # 3'ten 2'ye

document-processing-service:
  command: python -m uvicorn main_new:app --host 0.0.0.0 --port 8080 --workers 3  # 4'ten 3'e
```

### 4. Swap KullanÄ±mÄ±
8GB RAM yetersizse, swap ekleyin:
```bash
# 4GB swap ekle
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# KalÄ±cÄ± yap
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

## ğŸ“ˆ 20 KullanÄ±cÄ± Ä°Ã§in RAM KullanÄ±mÄ±

### Normal KullanÄ±m
- **EÅŸzamanlÄ± kullanÄ±cÄ±**: 10-15
- **RAM kullanÄ±mÄ±**: ~6-8GB
- **8GB sunucu**: âš ï¸ **SÄ±nÄ±rda**

### YoÄŸun KullanÄ±m
- **EÅŸzamanlÄ± kullanÄ±cÄ±**: 20
- **RAM kullanÄ±mÄ±**: ~8-10GB
- **8GB sunucu**: âŒ **Yetersiz**

## âœ… Ã–neri

### SeÃ§enek 1: RAM YÃ¼kselt (Ã–NERÄ°LEN)
**16GB RAM'e yÃ¼kseltin:**
- âœ… Rahat buffer (~6-8GB)
- âœ… 20 kullanÄ±cÄ± iÃ§in sorunsuz
- âœ… Gelecek iÃ§in hazÄ±r
- âœ… Swap'e gerek yok

### SeÃ§enek 2: Optimize Et (GeÃ§ici)
**8GB'da kal, optimize et:**
- âš ï¸ Resource limits'i dÃ¼ÅŸÃ¼r
- âš ï¸ Worker sayÄ±larÄ±nÄ± azalt
- âš ï¸ Ollama kullanma (cloud LLM kullan)
- âš ï¸ Swap ekle
- âš ï¸ 10-15 kullanÄ±cÄ± ile sÄ±nÄ±rla

### SeÃ§enek 3: Hibrit
**8GB + Optimizasyon:**
- Ollama'yÄ± kapat (cloud LLM kullan)
- Resource limits'i optimize et
- Worker sayÄ±larÄ±nÄ± azalt
- 15 kullanÄ±cÄ± ile test et

## ğŸš€ HÄ±zlÄ± Test

```bash
# Mevcut RAM kullanÄ±mÄ±nÄ± kontrol et
free -h

# Docker container RAM kullanÄ±mÄ±nÄ± gÃ¶r
docker stats --no-stream

# Toplam kullanÄ±mÄ± hesapla
docker stats --no-stream --format "table {{.Name}}\t{{.MemUsage}}"
```

## ğŸ“Š SonuÃ§

**8GB RAM ile:**
- âœ… Cloud LLM kullanÄ±yorsanÄ±z: **Yeterli** (5-7GB kullanÄ±m)
- âš ï¸ Ollama kÃ¼Ã§Ã¼k model: **SÄ±nÄ±rda** (7-9GB kullanÄ±m)
- âŒ Ollama orta/bÃ¼yÃ¼k model: **Yetersiz** (10GB+ kullanÄ±m)

**20 kullanÄ±cÄ± iÃ§in Ã¶neri:**
- **16GB RAM** (en gÃ¼venli)
- Veya **8GB + optimizasyon** (cloud LLM, dÃ¼ÅŸÃ¼k worker sayÄ±sÄ±)










